<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Is your Go application FIPS compliant?</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/31/your-go-application-fips-compliant" /><author><name>Antonio Cardace</name></author><id>34f13f47-1141-4739-b60e-a6c4321d8297</id><updated>2022-05-31T07:40:00Z</updated><published>2022-05-31T07:40:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) ships with several Federal Information Processing Standards (FIPS)-validated cryptography libraries, including OpenSSL. This allows applications that use these libraries to operate in &lt;em&gt;FIPS mode,&lt;/em&gt; which means that the cryptographic techniques they use can are in compliance with the &lt;a href="https://www.encryptionconsulting.com/education-center/what-is-fips/"&gt;FIPS-140-2 standard&lt;/a&gt;. Any organization that works with the U.S. Federal government must comply with this standard.&lt;/p&gt; &lt;p&gt;By default, applications written in &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; use cryptographic functions from the Go standard library, which is not FIPS-validated. However, the version of Go shipped in RHEL is based on upstream Go's dev.boringcrypto branch, which is modified to use BoringSSL for crypto primitives. Modifications made in the RHEL version replace BoringSSL with OpenSSL. These modifications allow applications written with RHEL's Go to use crypto functions from a FIPS-validated version of OpenSSL. This article will show you how to verify that your system, including your installation of the Go language, is capable of operating in FIPS mode.&lt;/p&gt; &lt;h2&gt;How to get started&lt;/h2&gt; &lt;p&gt;Begin by building your Go binary with the Go compiler shipped in RHEL. To do this quickly use the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/ubi-minimal/5c359a62bed8bd75a2c3fba8?gti-tabs=unauthenticated&amp;container-tabs=gtiimage"&gt;ubi8-minimal&lt;/a&gt; &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Universal Base Image&lt;/a&gt; (UBI) as your build environment and install the &lt;code&gt;go-toolset&lt;/code&gt; package.&lt;/p&gt; &lt;p&gt;To confirm that the Go compiler and built binary are FIPS-capable, run the command below, modified as appropriate for your environment (e.g, &lt;code&gt;$BINARY&lt;/code&gt; could be &lt;code&gt;/usr/bin/go&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ go tool nm $BINARY | grep FIPS&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If the output looks like the listing below, with references to named &lt;code&gt;FIPS&lt;/code&gt; functions, then the binary is FIPS-capable. (If it weren't FIPS-capable, the output would be empty.)&lt;/p&gt; &lt;pre&gt; &lt;code&gt; [root@rhel-8-4 ~]# go tool nm ./main | grep FIPS 401210 T _cgo_23e85cd750d7_Cfunc__goboringcrypto_FIPS_mode 5d8d80 d _g_FIPS_mode 4c2680 T crypto/internal/boring._Cfunc__goboringcrypto_FIPS_mode 5a27c0 D crypto/internal/boring._cgo_23e85cd750d7_Cfunc__goboringcrypto_FIPS_mode &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Next, run the application in a container that includes a FIPS-compliant OpenSSL library. Again, you can use the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/ubi-minimal/5c359a62bed8bd75a2c3fba8?gti-tabs=unauthenticated&amp;container-tabs=gtiimage"&gt;ubi8-minimal image&lt;/a&gt;, which fulfills the requirement. If you're using another image, you can check to see if the OpenSSL installation is FIPS-capable with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ openssl version OpenSSL 1.1.1k FIPS 25 Mar 2021 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Note: This can be disabled by enforcing pure Go with a build time flag.&lt;/p&gt; &lt;p&gt;As we noted above, the version of Go that ships with RHEL is based on the upstream Go's dev.boringcrypto branch, which is modified to use BoringSSL. You can verify this by looking at the source code either in the Go source RPM file or in Fedora's source code manager.&lt;/p&gt; &lt;p&gt;Grepping over the Go source code shows the first signs of the patched-in BoringSSL support:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ grep -r boring /usr/lib/golang/src/crypto/ | head /usr/lib/golang/src/crypto/aes/cipher.go:import "crypto/internal/boring" /usr/lib/golang/src/crypto/aes/cipher.go: if boring.Enabled() { &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The &lt;a href="https://pagure.io/go/blob/go1.15-openssl-fips/f/src/crypto/internal/boring/boring.go#_14"&gt;crypto/internal/boring package&lt;/a&gt; includes directives that use CGO to dynamically link against &lt;code&gt;libdl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; package boring // #include "goboringcrypto.h" // #cgo LDFLAGS: -ldl import "C" &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Linking against &lt;code&gt;libdl&lt;/code&gt; allows the use of the &lt;code&gt;dl_open()&lt;/code&gt; function, which allows for further loading of shared libraries. The &lt;code&gt;dl_open()&lt;/code&gt; function is called to &lt;a href="https://pagure.io/go/blob/go1.15-openssl-fips/f/src/crypto/internal/boring/goopenssl.h#_52"&gt;load libcrypto&lt;/a&gt;, one of the shared libraries in OpenSSL:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; static void* _goboringcrypto_DLOPEN_OPENSSL(void) { if (handle) { return handle; } #if OPENSSL_VERSION_NUMBER &lt; 0x10100000L handle = dlopen("libcrypto.so.10", RTLD_NOW | RTLD_GLOBAL); #else handle = dlopen("libcrypto.so.1.1", RTLD_NOW | RTLD_GLOBAL); #endif return handle; } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Because &lt;code&gt;dl_open()&lt;/code&gt; is used to load the OpenSSL libraries, rather than linking the actual Go binaries, &lt;code&gt;libcrypto&lt;/code&gt; will not appear in the output of &lt;code&gt;ldd&lt;/code&gt; when run on binaries built with RHEL Go (though &lt;code&gt;libdl&lt;/code&gt; will).&lt;/p&gt; &lt;p&gt;Although &lt;code&gt;libdl&lt;/code&gt; is linked, OpenSSL is not used by default. The &lt;a href="https://pagure.io/go/blob/go1.15-openssl-fips/f/src/crypto/internal/boring/boring.go#_55"&gt;crypto/internal/boring package&lt;/a&gt; will always load OpenSSL, but it will only use it if FIPS mode is enabled:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; func init() { runtime.LockOSThread() defer runtime.UnlockOSThread() // Check if we can `dlopen` OpenSSL if C._goboringcrypto_DLOPEN_OPENSSL() == C.NULL { return } // Initialize the OpenSSL library. C._goboringcrypto_OPENSSL_setup() // Check to see if the system is running in FIPS mode, if so // enable "boring" mode to call into OpenSSL for FIPS compliance. if fipsModeEnabled() { enableBoringFIPSMode() } sig.BoringCrypto() } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;FIPS_mode_set()&lt;/code&gt; and &lt;code&gt;FIPS_mode()&lt;/code&gt; functions are defined in OpenSSL, which is why &lt;code&gt;libcrypto&lt;/code&gt; is loaded even if it won't be used for crypto functions. OpenSSL is used to check if the system is in FIPS mode.&lt;/p&gt; &lt;h2&gt;How to verify FIPS mode&lt;/h2&gt; &lt;p&gt;Depending on how deep you want to go, there are a couple of different ways in which you can check for FIPS compliance.&lt;/p&gt; &lt;h3&gt;Custom fips-detect tool&lt;/h3&gt; &lt;p&gt;A tool called &lt;a href="https://github.com/acardace/fips-detect"&gt;fips-detect&lt;/a&gt; is available to determine whether your system or container and your Golang binary are ready to run in FIPS mode. It accomplishes this by performing checks on the running system and the supplied binary to see if everything is in place to correctly run in FIPS mode.&lt;/p&gt; &lt;h3&gt;Common tools&lt;/h3&gt; &lt;p&gt;If you don't want to install a custom tool, you can use &lt;code&gt;ldd&lt;/code&gt; to show that Go binaries are linked against &lt;code&gt;libdl&lt;/code&gt;, and inspect the source code to determine if &lt;code&gt;dl_open()&lt;/code&gt; is used to load OpenSSL. There are a couple of options available, including &lt;code&gt;go tool nm&lt;/code&gt; or &lt;code&gt;readelf -s&lt;/code&gt;. However, keep in mind that no checks on the underlying system are performed with these common tools, and it may not be convincing enough to inspect the Go binaries alone.&lt;/p&gt; &lt;p&gt;If the binary is compiled on a standard Fedora 34 system, when you enter this &lt;code&gt;go tool nm&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ go tool nm ./main | grep -i dlopen_openssl &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The output you'll get will be blank. If the binary is compiled on RHEL 8, however, this is the result:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ go tool nm ./main | grep -i dlopen_openssl [root@rhel-8-4 ~]# go tool nm ./main | grep -i dlopen_openssl 4018d0 T _cgo_fb383f177a95_Cfunc__goboringcrypto_DLOPEN_OPENSSL &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;From this output, it is clear that, when compiling with the RHEL Golang compiler, the binary is at least able to call into OpenSSL and enable FIPS mode.&lt;/p&gt; &lt;p&gt;To verify that the program runs in FIPS mode, use the &lt;code&gt;LD_DEBUG=symbols&lt;/code&gt; environment variable. This shows the various symbols the binary binds from shared libraries to determine whether the application actually calls into OpenSSL.&lt;/p&gt; &lt;p&gt;The binary runs in FIPS mode when executed with the &lt;code&gt;OPENSSL_FORCE_FIPS_MODE=1&lt;/code&gt; variable. In the following example, we use a custom Go binary that computes a hash using the SHA1 algorithm:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; [root@rhel-8-4 ~]# env OPENSSL_FORCE_FIPS_MODE=1 ./main 5939: symbol=FIPS_mode; lookup in file=./main [0] 5939: symbol=FIPS_mode; lookup in file=/lib64/libssl.so.1.1 [0] 5939: symbol=FIPS_mode; lookup in file=/lib64/libcrypto.so.1.1 [0] 5939: symbol=SHA1_Init; lookup in file=./main [0] 5939: symbol=SHA1_Init; lookup in file=/lib64/libssl.so.1.1 [0] 5939: symbol=SHA1_Init; lookup in file=/lib64/libcrypto.so.1.1 [0] 5939: symbol=SHA1_Update; lookup in file=./main [0] 5939: symbol=SHA1_Update; lookup in file=/lib64/libssl.so.1.1 [0] 5939: symbol=SHA1_Update; lookup in file=/lib64/libcrypto.so.1.1 [0] 5939: symbol=SHA1_Final; lookup in file=./main [0] 5939: symbol=SHA1_Final; lookup in file=/lib64/libssl.so.1.1 [0] 5939: symbol=SHA1_Final; lookup in file=/lib64/libcrypto.so.1.1 [0] SHA1: C8282111D0FAD11680B3775A36E68DC41E36F911 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;For comparison, this is the result when it runs without this variable:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; [root@rhel-8-4 ~]# ./main 12957: symbol=FIPS_mode_set; lookup in file=/lib64/libcrypto.so.1.1 [0] 12957: symbol=dlsym; lookup in file=./main [0] 12957: symbol=dlsym; lookup in file=/lib64/libdl.so.2 [0] 12957: symbol=OPENSSL_init; lookup in file=/lib64/libcrypto.so.1.1 [0] 12957: symbol=FIPS_mode; lookup in file=/lib64/libcrypto.so.1.1 [0] SHA1: C8282111D0FAD11680B3775A36E68DC41E36F911 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The differences are:&lt;/p&gt; &lt;ul&gt; &lt;li aria-level="1"&gt;When the binary runs in non-FIPS mode, it uses the Golang standard crypto library, which uses its own routines (these are statically included in the binary) rather than calling into OpenSSL. This is why the dynamic linker doesn't bind OpenSSL symbols.&lt;/li&gt; &lt;li aria-level="1"&gt;OpenSSL is still loaded even when FIPS mode is disabled. It provides the functions used to check if FIPS mode is enabled or disabled. Only enabled FIPS mode uses OpenSSL for cryptographic functions.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;FIPS mode&lt;/h2&gt; &lt;p&gt;Earlier, we used the &lt;code&gt; OPENSSL_FORCE_FIPS_MODE&lt;/code&gt; environment variable to force the binary to behave as if FIPS mode were enabled.&lt;/p&gt; &lt;p&gt;The following is a snippet of the &lt;code&gt;libcrypto.so&lt;/code&gt; shared library constructor. It runs in the lib init phase before the dynamic linker transfers the flow control to &lt;code&gt;main()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; # define FIPS_MODE_SWITCH_FILE "/proc/sys/crypto/fips_enabled" static void init_fips_mode(void) { char buf[2] = "0"; int fd; if (secure_getenv("OPENSSL_FORCE_FIPS_MODE") != NULL) { buf[0] = '1'; } else if ((fd = open(FIPS_MODE_SWITCH_FILE, O_RDONLY)) &gt;= 0) { while (read(fd, buf, sizeof(buf)) &lt; 0 &amp;&amp; errno == EINTR) ; close(fd); } if (buf[0] != '1' &amp;&amp; !FIPS_module_installed()) return; FIPS_mode_set(1); if (buf[0] != '1') { /* drop down to non-FIPS mode if it is not requested */ FIPS_mode_set(0); } else { /* abort if selftest failed */ FIPS_selftest_check(); } } void __attribute__ ((constructor)) OPENSSL_init_library(void) { static int done = 0; if (done) return; done = 1; init_fips_mode(); } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This snippet comes from a patch included in the RHEL and Fedora versions of OpenSSL, so it only applies to RHEL and the UBI container image.&lt;/p&gt; &lt;p&gt;The shared library constructor shows that, to transparently enable FIPS mode, you must either define the &lt;code&gt;OPENSSL_FORCE_FIPS_MODE&lt;/code&gt; variable or ensure that the first byte of the &lt;code&gt;/proc/sys/crypto/fips_enabled&lt;/code&gt; file contains 1.&lt;/p&gt; &lt;p&gt;Containers will detect hosts that are in FIPS mode when &lt;code&gt;/proc/sys/crypto/fips_enabled&lt;/code&gt; appears with the host's value in all container PID namespaces.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Building Go applications on RHEL allows them to run in two different modes, default and FIPS. The default mode uses the Go standard library, and the FIPS mode uses a FIPS-validated version of OpenSSL. This provides developers an easy way to meet compliance requirements that mandate the use of FIPS-validated libraries, while also preserving consistency with applications built upstream.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/31/your-go-application-fips-compliant" title="Is your Go application FIPS compliant?"&gt;Is your Go application FIPS compliant?&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Antonio Cardace</dc:creator><dc:date>2022-05-31T07:40:00Z</dc:date></entry><entry><title>Integrate a Spring Boot application with Red Hat Data Grid</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/31/integrate-spring-boot-application-red-hat-data-grid" /><author><name>Alexander Barbosa Ayala</name></author><id>fd568e80-1fd7-4d83-be98-53ee6d73e237</id><updated>2022-05-31T07:00:00Z</updated><published>2022-05-31T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/data-grid" target="_blank"&gt;Red Hat Data Grid&lt;/a&gt; is a middleware solution that has been developed for application cache data storage. It makes it possible to access and process in-memory data, improving the end-user experience.&lt;/p&gt; &lt;p&gt;This article offers guidance for Spring Boot and Red Hat Data Grid integration on version 4.9 of the &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. You will set up a Red Hat Data Grid 8.2 cluster and deploy a Spring Boot application with separate namespaces to use &lt;a href="https://infinispan.org/docs/stable/titles/hotrod_java/hotrod_java.html"&gt;Hot Rod&lt;/a&gt; communication between them.&lt;/p&gt; &lt;h2&gt;Setting up the environment&lt;/h2&gt; &lt;p&gt;The integration described in this article was made using the following technologies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Red Hat Data Grid 8.2&lt;/li&gt; &lt;li&gt;OpenShift 4.9&lt;/li&gt; &lt;li&gt;Spring Boot 2.5&lt;/li&gt; &lt;li&gt;Java 11&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Deploy the Data Grid cluster&lt;/h2&gt; &lt;p&gt;Refer to the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.2/html-single/data_grid_operator_guide/index#creating-minimal-clusters_infinispan-cr"&gt;Data Grid Operator Guide&lt;/a&gt; for step-by-step instructions on deploying Red Hat Data Grid. For this integration, the Data Grid project's name is &lt;code&gt;dgtest&lt;/code&gt;, and the Infinispan cluster's name is &lt;code&gt;infinispan-test&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Create the &lt;code&gt;dgtest&lt;/code&gt; project:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc new-project dgtest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then create a new Data Grid cluster using the Operator. For this integration, we created a custom resource YAML file using the Operator option &lt;strong&gt;Create Infinispan.&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;apiVersion: infinispan.org/v1 kind: Infinispan metadata: name: infinispan-test namespace: dgtest spec: expose: type: Route service: type: DataGrid replicas: 2&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Gather relevant Data Grid cluster data&lt;/h2&gt; &lt;p&gt;To integrate the Data Grid cluster for external use, you'll need to save some cluster data, such as the developer user credentials, the cluster CRT certificate, and the local SVC DNS hostname.&lt;/p&gt; &lt;p&gt;Begin by getting the developer user credentials.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;oc get secret infinispan-test-generated-secret \ -o jsonpath="{.data.identities\.yaml}" | base64 --decode &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The credentials will look something like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;credentials: - username: developer password: SYtaUwZfPzNjHYeC roles: - admin &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now you need to get the data grid cluster certificate, &lt;code&gt;tls.crt&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;oc get secret infinispan-test-cert-secret \  -o jsonpath='{.data.tls\.crt}' | base64 --decode &gt; tls.crt &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Finally, get the service DNS hostname SVC for internal OpenShift routing.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;oc get service infinispan-test -o go-template --template='{{.metadata.name}}.{{.metadata.namespace}}.svc.cluster.local{{println}}' &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The result should be something like &lt;code&gt;infinispan-test.dgtest.svc.cluster.local&lt;/code&gt;, which is what we'll use as the hostname for the purposes of this article.&lt;/p&gt; &lt;h2&gt;Create a new cache&lt;/h2&gt; &lt;p&gt;For this integration, the application must be able to store and read cache data from a cache named &lt;code&gt;sessions&lt;/code&gt;. You can use &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.2/html/data_grid_server_guide/create_remote_cache" target="_blank"&gt;multiple methods&lt;/a&gt; to create the cache, including the command-line interface (CLI) and the Web Management console. This article will use the Data Grid CLI method, connecting through the exposed route and the &lt;code&gt;developer&lt;/code&gt; user credentials:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;./opt/infinispan/bin/cli.sh [disconnected]&gt; connect https://infinispan-test-dgtest.openshiftcluster.com/ --trustall Username: developer Password: **************** [infinispan-test-0-26062@infinispan-test//containers/default]&gt; create cache sessions --template=org.infinispan.DIST_SYNC [infinispan-test-0-26062@infinispan-test//containers/default]&gt; [infinispan-test-0-26062@infinispan-test//containers/default]&gt; quit &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The generated cache is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat /opt/infinispan/server/data/caches.xml &lt;?xml version="1.0"?&gt; &lt;infinispan xmlns="urn:infinispan:config:13.0"&gt; &lt;cache-container&gt; &lt;caches&gt; &lt;distributed-cache name="sessions" mode="SYNC" remote-timeout="17500" statistics="true"&gt; &lt;locking concurrency-level="1000" acquire-timeout="15000" striping="false"/&gt; &lt;state-transfer timeout="60000"/&gt; &lt;/distributed-cache&gt; &lt;/caches&gt; &lt;/cache-container&gt; &lt;/infinispan&gt; &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;How to deploy the Spring Boot project&lt;/h2&gt; &lt;p&gt;The next step is to clone &lt;a href="https://github.com/alexbarbosa1989/hotrodspringboot"&gt;hotrodspringboot&lt;/a&gt;, a project that I have made available on my GitHub repository. This is a basic project that stores and retrieves cache data from the Data Grid cluster. It is deployed in a separate OpenShift namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone -b openshift https://github.com/alexbarbosa1989/hotrodspringboot &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You then need to create a truststore from the &lt;code&gt;tls.crt&lt;/code&gt; cluster certificate.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;keytool -importcert -keystore truststore.jks -alias server -file tls.crt &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The data from &lt;code&gt;truststore.jks&lt;/code&gt; allows the Spring Boot application to create the secure connection to the Data Grid cluster.&lt;/p&gt; &lt;p&gt;The sample Spring Boot application comes with a set of variables in the &lt;code&gt;application.properties&lt;/code&gt; file that must be replaced by the data previously gathered to make a correct connection with the Data Grid cluster. That &lt;code&gt;application.properties&lt;/code&gt; file is located in the &lt;code&gt;${project-home}/src/main/resources/&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;So the next step is to update the variables in &lt;code&gt;application.properties&lt;/code&gt; file according to the Data Grid cluster data:&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="706"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th scope="col"&gt;Variable&lt;/th&gt; &lt;th scope="col"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$SERVICE_HOSTNAME&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;infinispan-test.dgtest.svc.cluster.local&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$CLUSTER_NAME&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;infinispan-test&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$USER_NAME&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;developer&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$USER_PASSWORD&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;SYtaUwZfPzNjHYeC&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$TRUST_STORE_FILE_PATH&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;/mnt/secrets/truststore.jks&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$TRUST_STORE_PASSWORD&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;password&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Once you've customized the &lt;code&gt;application.properties&lt;/code&gt; file with those mapped values, it should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt; # context-path server.servlet.context-path=/redhat # Connection infinispan.remote.server-list=infinispan-test.dgtest.svc.cluster.local infinispan.remote.client-intelligence=BASIC management.endpoints.web.exposure.include=* # Authentication infinispan.remote.use-auth=true infinispan.remote.sasl-mechanism=SCRAM-SHA-512 infinispan.remote.auth-realm=default infinispan.remote.auth-server-name=infinispan-test infinispan.remote.auth-username=developer infinispan.remote.auth-password=SYtaUwZfPzNjHYeC infinispan.remote.sasl_properties.javax.security.sasl.qop=auth # Encryption infinispan.remote.sni_host_name=infinispan-test.dgtest.svc.cluster.local infinispan.remote.trust_store_file_name=/mnt/secrets/truststore.jks infinispan.remote.trust_store_password=password infinispan.remote.trust_store_type=jks # Marshalling infinispan.remote.marshaller=org.infinispan.commons.marshall.JavaSerializationMarshaller infinispan.remote.java-serial-whitelist=com.redhat.hotrod.* &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Create a new project in OpenShift for Spring Boot application deployment&lt;/h2&gt; &lt;p&gt;Now it's time to create an OpenShift project for your Spring Boot application. Create a new project on your OpenShift cluster named &lt;code&gt;springboot-test&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc new-project springboot-test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create a secret with the previously created &lt;code&gt;truststore.jks&lt;/code&gt; file in the &lt;code&gt;springboot-test&lt;/code&gt; project:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc create secret generic truststore-secret --from-file=truststore.jks &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploy the Spring Boot application&lt;/h2&gt; &lt;p&gt;Now you're ready to deploy the application on the &lt;code&gt;springboot-test&lt;/code&gt; OpenShift project.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc project springboot-test mvn clean fabric8:deploy -Popenshift &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Wait until the pod is up and running.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pods class="language-bash" NAME READY STATUS RESTARTS AGE hotrodspringboot-1-plkf7 1/1 Running 0 39s hotrodspringboot-s2i-1-build 0/1 Completed 0 92s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to add the secret to the deployment configuration. In this case, the mount path, &lt;code&gt;/mnt/secrets&lt;/code&gt;, must be the same as the one defined in the &lt;code&gt;application.properties&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc set volume dc/hotrodspringboot --add --name=truststore-secret -m /mnt/secrets/ -t secret --secret-name=truststore-secret --default-mode='0755'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can load the secret using the Openshift web console by navigating to &lt;strong&gt;Projects -&gt; springboot-test -&gt; secrets -&gt; ap-secret -&gt; Add Secret to workload&lt;/strong&gt; (Figure 1).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_8.png?itok=NONq2tEL" width="600" height="304" alt="Screenshoot showing how to add a secret to the deployment configuration." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Add the secret to the deployment configuration. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;This process generates a new pod deployment after the new pod rollout is finished.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pods NAME READY STATUS RESTARTS AGE hotrodspringboot-2-deploy 0/1 Completed 0 100m hotrodspringboot-2-x7lgx 1/1 Running 0 100m hotrodspringboot-s2i-1-build 0/1 Completed 0 154m hotrodspringboot-s2i-2-build 0/1 Completed 0 102m &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Test the deployed integration&lt;/h2&gt; &lt;p&gt;In order to test the service, you need to get the Spring Boot exposed route.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get routes NAME HOST/PORT hotrodspringboot hotrodspringboot-springboot-test.openshiftcluster.com PATH SERVICES PORT TERMINATION WILDCARD hotrodspringboot 8080 None &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Next, you need to create a new cache entry, which takes the form of a key/value pair. This application uses a REST endpoint to get the data to store in the cache. The context path used for the update service is &lt;code&gt; /update-cache/{cacheName}/{cacheKey}/{cacheValue}&lt;/code&gt;. Here is an example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X GET http://hotrodspringboot-springboot-test.openshiftcluster.com/redhat/update-cache/sessions/cacheKey1/cacheValue1 SUCCESS cacheValue1&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You can repeat this process to create a second cache entry or however many key/value pairs that you want for your test:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X GET http://hotrodspringboot-springboot-test.openshiftcluster.com/redhat/update-cache/sessions/cacheKey2/cacheValue2 SUCCESS cacheValue2&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now you can get the values from any of the previously stored cache data. As was the case with the update service, the application contains a REST endpoint for querying data from the cache. You can make a query using the key name from any of the previously stored key/value pairs using the context path &lt;code&gt; /get-cache-value/{cacheName}/{cacheKey}&lt;/code&gt;, as in this example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X GET http://hotrodspringboot-springboot-test.openshiftcluster.com/redhat/get-cache-value/sessions/cacheKey1 The value for key: cacheKey1 is: cacheValue1&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;At this point, the Data Grid session cache stores both cache entries. You can also verify the entries on the Data Grid side.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[infinispan-test-0-26062@infinispan-test//containers/default]&gt; cache sessions [infinispan-test-0-26062@infinispan-test//containers/default/caches/sessions]&gt; stats { "time_since_start" : 5364, "time_since_reset" : 5364, "current_number_of_entries" : -1, "current_number_of_entries_in_memory" : -1, "total_number_of_entries" : 2, "off_heap_memory_used" : 0, "data_memory_used" : 0, "stores" : 2, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can continue storing and querying data using these methods.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Data Grid integration allows applications to store and quickly use data externally. This example is a simple use case that can also apply to larger data sets across cloud and hybrid platforms. The official &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/html/data_grid_server_guide/index" target="_blank"&gt;product&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/html/data_grid_operator_guide/index" target="_blank"&gt;Operator&lt;/a&gt; documentation provide more Data Grid features to fit your use case.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/31/integrate-spring-boot-application-red-hat-data-grid" title="Integrate a Spring Boot application with Red Hat Data Grid"&gt;Integrate a Spring Boot application with Red Hat Data Grid&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alexander Barbosa Ayala</dc:creator><dc:date>2022-05-31T07:00:00Z</dc:date></entry><entry><title type="html">Banco do Brasil extracts Open Banking investment data with Quarkus and Kafka</title><link rel="alternate" href="https://quarkus.io/blog/banco-do-brasil-open-banking-user-story/" /><author><name>Felipe Henrique Gross Windmoller</name></author><id>https://quarkus.io/blog/banco-do-brasil-open-banking-user-story/</id><updated>2022-05-31T00:00:00Z</updated><content type="html">Banco do Brasil S.A. is a Brazilian financial services company headquartered in Brasília, Brazil. The oldest bank in Brazil, and among the oldest banks in continuous operation in the world, it was founded by John VI, King of Portugal, in 1808. It is the second largest banking institution in Brazil,...</content><dc:creator>Felipe Henrique Gross Windmoller</dc:creator></entry><entry><title type="html">Infinispan 14.0.0.Dev03</title><link rel="alternate" href="https://infinispan.org/blog/2022/05/30/infinispan-14" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2022/05/30/infinispan-14</id><updated>2022-05-30T12:00:00Z</updated><content type="html">Dear Infinispan community, Infinispan 14 development release 03 is here! We plan to release Infinispan 14 Final this summer, so we want to share a preview of what’s coming. JAVA 11 Infinispan now requires Java 11 to run (it was only needed to build it before). This means we can use and expose all of the great new APIs that were added, such as java.util.concurrent.Flow which provides a standard interface for all things reactive. Which brings us to the… NEW API We have finalized the design of our new user-facing API, which brings the following, much-needed, features: * a common API for both embedded and remote * clean separation between sync and async APIs, as well as a variant which blends beautifully with all the great things happening over in . * a single entry-point to access all of the data-structures that we support (caches, counters, locks, multimaps, etc) * our own annotations for indexing entity fields (see below for details) We are now working on implementing this API for the remote Hot Rod client, while the implementation for embedded will be available in Infinispan 15. JGROUPS 5 Upgrading to Java 11 also allows us to upgrade to JGroups 5.x, which brings a bunch of improvements: * Improved failure-detection protocols (FD_ALL3, FD_SOCK2) * The Random Early Drop protocol (RED), which starts dropping messages on the send side when the queue becomes full to prevent message storms caused by unneeded retransmissions. * Lots more. GROUPING The grouping API has a small improvement when searching for keys belonging to a group. The old code was inefficient because it iterates over all keys in the local nodes but that was changed in this release by iterating over a single segment. TRANSACTIONAL CACHES The internal codeis now non-blocking, reducing the overall threads spawning and making better use of resources when transactions are committed. CROSS-SITE REPLICATION The asynchronous cross-site replication updates are batched in the sender improving the overall resources utilization. JAKARTA EE JavaEE is dead. Long-live . Wherever we used to depend on javax APIs, we now depend on their jakarta equivalent. We still provide compatibility artifacts for legacy deployments. HIBERNATE ORM 6.0 COMPATIBILITY Infinispan’s Hibernate ORM second-level cache (2LC) implementation has been upgraded to work with Hibernate 6.0. INDEXING AND QUERY Lots has been happening in the land of indexing and querying. * Upgraded Hibernate Search to 6.1 and Lucene 8.11. * Brand-new annotations for indexing annotations in place of the old Hibernate annotations. * Schema index update to acquire ProtoBuf schema backward-compatible changes without touching the pre-existing index data. * The removal of the @ProtoDoc annotation to wrap indexing annotations for ProtoBuf generation. * New indexing startup mode configuration, to trigger purge or reindex automatically when the cache starts. * Support pagination for unbounded result size queries with the HotRod client. * Support query parameters for full-text analyzed fields. * Support normalizers with the HotRod client. * Improve the Hybrid query system. MICROMETER We’ve replaced our use of SmallRye Metrics (an implementation of Microprofile Metrics), with the much better Micrometer. SERVER * RESP endpoint: a Redis-compatible endpoint connector (implementing the RESP 3 protocol) with support for a subset of commands: set, get, del, mget, mset, incr, decr, publish, subscribe, auth, ping. The connector integrates with our security and protocol auto-detections, so that it is easily usable from our single-port endpoint. The implemented commands should be enough for basic usage. If you would like to see more, reach out via our community. * Support for FIPS environments (PKCS#11) * Support for masked and external credentials CONSOLE The console now sports a cache-creation wizard: a feature-driven approach to configuring caches just the way you need them. DOCUMENTATION As always, the Infinispan team hope you find the documentation useful and complete. We’d love to hear from you and really value feedback from our community. If you think something is missing from the docs or spot a correction, please get in touch and we’ll get on it straight away. RELEASE NOTES You can look at the to see what has changed. OTHER RELEASES We’ve also just recently updated our stable releases with important fixes: * * Get them from our .</content><dc:creator>Tristan Tarrant</dc:creator></entry><entry><title>Red Hat Developer roundup: Best of May 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/30/red-hat-developer-roundup-best-may-2022" /><author><name>Red Hat Developer Editorial Team</name></author><id>5c132813-fe79-494b-8008-ce380725f246</id><updated>2022-05-30T07:00:00Z</updated><published>2022-05-30T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome to our monthly recap of the articles we published in May! In case you missed it, we had some important product announcements this month that deserve your attention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/26/orchestrate-offloaded-network-functions-dpus-red-hat-openshift"&gt;Orchestrate offloaded network functions on DPUs with Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/whats-new-openshift-local-20"&gt;What’s new in OpenShift Local 2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/access-rhel-developer-teams-subscription"&gt;Access RHEL with a Developer for Teams subscription&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/introducing-red-hat-openshift-extension-docker-desktop"&gt;Introducing Red Hat OpenShift extension for Docker Desktop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/11/rhel-86-whats-new-and-how-upgrade"&gt;RHEL 8.6: What's new and how to upgrade&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/12/developer-tools-rebrand-say-farewell-codeready-name"&gt;Developer tools rebrand, say farewell to CodeReady name&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/whats-new-red-hat-enterprise-linux-9"&gt;What's new in Red Hat Enterprise Linux 9&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/whats-new-ansible-automation-platform-22"&gt;What's new in Ansible Automation Platform 2.2&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And of course, we rolled out a plethora of articles to help you write code on the platforms you trust. Here are the May highlights.&lt;/p&gt; &lt;ul&gt; &lt;/ul&gt; &lt;h2&gt;Understanding the Kafka landscape&lt;/h2&gt; &lt;p&gt;Bilgim Ibryam unleashed a series of popular Kafka articles this month, starting with &lt;a href="https://developers.redhat.com/articles/2022/05/03/fine-tune-kafka-performance-kafka-optimization-theorem"&gt;Fine-tune Kafka performance with the Kafka optimization theorem&lt;/a&gt;, which analyzes the options you need to consider with any Kafka rollout, balancing latency against throughput and durability against availability.&lt;/p&gt; &lt;p&gt;He also delivered a two-part series analyzing the different types of Kafka distribution on the landscape, looking at both &lt;a href="https://developers.redhat.com/articles/2022/05/16/all-about-local-and-self-managed-kafka-distributions"&gt;local and self-managed Kafka distributions&lt;/a&gt; as well as &lt;a href="https://developers.redhat.com/articles/2022/05/24/managed-kafka-services-which-right-you"&gt;managed services&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To get the latest on what's new in the Kafka world, be sure to check our &lt;a href="https://developers.redhat.com/articles/2022/05/09/kafka-monthly-digest-april-2022"&gt;Kafka monthly digest&lt;/a&gt; series.&lt;/p&gt; &lt;h2&gt;OpenShift and AWS&lt;/h2&gt; &lt;p&gt;Kubernetes and &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift&lt;/a&gt; can play well with Amazon Web Services cloud environments. August Simonelli brought you a two-part series showing how to simplify the management of services offered for Kubernetes by AWS:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes"&gt;How to use Operators with AWS Controllers for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/24/create-aws-resources-kubernetes-and-operators"&gt;Create AWS resources with Kubernetes and Operators&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And for a deep dive on managing secure connections with OpenShift and AWS, read &lt;a href="https://developers.redhat.com/articles/2022/04/27/create-privatelink-red-hat-openshift-cluster-aws-sts"&gt;Create a PrivateLink Red Hat OpenShift cluster on AWS with STS&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;SaaS architectures&lt;/h2&gt; &lt;p&gt;This month we published the first two articles in a new series about building and deploying SaaS applications, with a focus on software and deployment architectures:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/09/approaches-implementing-multi-tenancy-saas-applications"&gt;A SaaS architecture checklist for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes"&gt;Approaches to implementing multi-tenancy in SaaS applications&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check these articles out and be on the lookout for future installments.&lt;/p&gt; &lt;h2&gt;Instrument containerized Java applications with Cryostat&lt;/h2&gt; &lt;p&gt;&lt;a href="https://cryostat.io"&gt;Cryostat&lt;/a&gt; is a tool for managing &lt;a href="https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170"&gt;JDK Flight Recorder&lt;/a&gt; data on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, and Cryostat 2.1 delivers a slew of improvements and new features. Check out this series on the topic to learn more:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/sso-all-cryostats-new-openshift-login-flow"&gt;How to log into Cryostat 2.1 on OpenShift: SSO for all&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/11/how-build-automated-jfr-rules-cryostat-21s-new-ui"&gt;How to build automated JFR rules with Cryostat 2.1's new UI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/12/how-organize-jfr-data-recording-labels-cryostat-21"&gt;How to organize JFR data with recording labels in Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/17/manage-jfr-across-instances-cryostat-and-graphql"&gt;Manage JFR across instances with Cryostat and GraphQL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/19/manage-jmx-credentials-kubernetes-cryostat-21"&gt;Manage JMX credentials on Kubernetes with Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/25/eat-fewer-resources-cryostat-21-sidecar-reports"&gt;Eat up fewer resources in Cryostat 2.1 with sidecar reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/25/access-jfr-data-faster-cryostat-21s-new-download-apis"&gt;Access JFR data faster with Cryostat 2.1's new download APIs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/filter-unwanted-notifications-cryostat-21"&gt;Filter unwanted notifications in Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;May 2022 on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Here's the full lineup of articles published on Red Hat Developer so far this month:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/26/orchestrate-offloaded-network-functions-dpus-red-hat-openshift"&gt;Orchestrate offloaded network functions on DPUs with Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/27/create-privatelink-red-hat-openshift-cluster-aws-sts"&gt;Create a PrivateLink Red Hat OpenShift cluster on AWS with STS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/28/use-red-hats-single-sign-technology-secure-services-through-kerberos"&gt;Use Red Hat's single sign-on technology to secure services through Kerberos&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/28/process-formula-1-telemetry-quarkus-and-openshift-streams-apache-kafka"&gt;Process Formula 1 telemetry with Quarkus and OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/02/no-code-and-low-code-integrations-camel-and-kaoto"&gt;No-code and low-code integrations with Camel and Kaoto&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/02/podman-basics-resources-beginners-and-experts"&gt;Podman basics: Resources for beginners and experts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/03/fine-tune-kafka-performance-kafka-optimization-theorem"&gt;Fine-tune Kafka performance with the Kafka optimization theorem&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/03/red-hat-cloud-way-event-driven-serverless-distributed-cloud-services-support"&gt;The Red Hat Cloud way: Event-driven, serverless, distributed cloud services to support modern apps&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/04/schedule-tests-gitops-way-testing-farm-github-action"&gt;Schedule tests the GitOps way with Testing Farm as GitHub Action&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/04/use-red-hats-sso-manage-kafka-broker-authorization"&gt;Use Red Hat's SSO to manage Kafka broker authorization&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/05/how-install-open-source-tool-creating-machine-learning-pipelines"&gt;How to install an open source tool for creating machine learning pipelines&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/05/build-customized-developer-portal-manage-apis"&gt;Build a customized developer portal to manage APIs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/09/using-unsafe-safely-graalvm-native-image"&gt;Using Unsafe safely in GraalVM Native Image&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/09/kafka-monthly-digest-april-2022"&gt;Kafka Monthly Digest: April 2022&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/sso-all-cryostats-new-openshift-login-flow"&gt;How to log into Cryostat 2.1 on OpenShift: SSO for all&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/introducing-red-hat-openshift-extension-docker-desktop"&gt;Introducing Red Hat OpenShift extension for Docker Desktop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/access-rhel-developer-teams-subscription"&gt;Access RHEL with a Developer for Teams subscription&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/sso-all-cryostats-new-openshift-login-flow"&gt;How to log into Cryostat 2.1 on OpenShift: SSO for all&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/6-design-tips-java-microservices-development"&gt;6 design tips for Java microservices development&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/improve-developer-experience-and-process-local-kubernetes"&gt;Improve the developer experience and process, from local to Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/manage-cluster-resources-efficiently-red-hat-openshift"&gt;Manage cluster resources efficiently with Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/access-rhel-developer-teams-subscription"&gt;Access RHEL with a Developer for Teams subscription&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/introducing-red-hat-openshift-extension-docker-desktop"&gt;Introducing Red Hat OpenShift extension for Docker Desktop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/11/how-build-automated-jfr-rules-cryostat-21s-new-ui"&gt;How to build automated JFR rules with Cryostat 2.1's new UI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/11/rhel-86-whats-new-and-how-upgrade"&gt;RHEL 8.6: What's new and how to upgrade&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/12/how-organize-jfr-data-recording-labels-cryostat-21"&gt;How to organize JFR data with recording labels in Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/12/developer-tools-rebrand-say-farewell-codeready-name"&gt;Developer tools rebrand, say farewell to CodeReady name&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes"&gt;How to use Operators with AWS Controllers for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/16/all-about-local-and-self-managed-kafka-distributions"&gt;All about local and self-managed Kafka distributions&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/17/manage-jfr-across-instances-cryostat-and-graphql"&gt;Manage JFR across instances with Cryostat and GraphQL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes"&gt;A SaaS architecture checklist for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/whats-new-red-hat-enterprise-linux-9"&gt;What's new in Red Hat Enterprise Linux 9&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/19/manage-jmx-credentials-kubernetes-cryostat-21"&gt;Manage JMX credentials on Kubernetes with Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/09/approaches-implementing-multi-tenancy-saas-applications"&gt;Approaches to implementing multi-tenancy in SaaS applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/23/how-install-command-line-tools-mac"&gt;How to install command-line tools on a Mac&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/23/plpmtud-delivers-better-path-mtu-discovery-sctp-linux"&gt;PLPMTUD delivers better path MTU discovery for SCTP in Linux&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/24/create-aws-resources-kubernetes-and-operators"&gt;Create AWS resources with Kubernetes and Operators&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/24/managed-kafka-services-which-right-you"&gt;Managed Kafka services: Which is right for you?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/25/eat-fewer-resources-cryostat-21-sidecar-reports"&gt;Eat up fewer resources in Cryostat 2.1 with sidecar reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/25/access-jfr-data-faster-cryostat-21s-new-download-apis"&gt;Access JFR data faster with Cryostat 2.1's new download APIs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/filter-unwanted-notifications-cryostat-21"&gt;Filter unwanted notifications in Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/whats-new-ansible-automation-platform-22"&gt;What’s new in Ansible Automation Platform 2.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/experiment-openshift-api-management-developer-sandbox"&gt;Experiment with the OpenShift API Management Developer Sandbox&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/30/red-hat-developer-roundup-best-may-2022" title="Red Hat Developer roundup: Best of May 2022"&gt;Red Hat Developer roundup: Best of May 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Red Hat Developer Editorial Team</dc:creator><dc:date>2022-05-30T07:00:00Z</dc:date></entry><entry><title type="html">How to run WildFly on Openshift</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/openshift/using-wildfly-on-openshift/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/openshift/using-wildfly-on-openshift/</id><updated>2022-05-30T00:38:00Z</updated><content type="html">This tutorial will teach you how to run WildFly applications on Openshift using WildFly S2I images. At first, we will learn how to build and deploy applications using Helm Charts. Then, we will learn how to use the S2I legacy approach which relies on ImageStreams and Templates. WildFly Cloud deployments There are two main strategies ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">New Keycloak certifications</title><link rel="alternate" href="https://www.keycloak.org/2022/05/oidc-certifications" /><author><name>Marek Posolda</name></author><id>https://www.keycloak.org/2022/05/oidc-certifications</id><updated>2022-05-30T00:00:00Z</updated><content type="html">We are glad to announce new certifications for Keycloak related to the and ! In the , we announced certification of Keycloak 15.0.2 with the FAPI and Brazil Open Banking. This is a follow-up of this post with the announcement of the additional certifications. Here are the details: * Keycloak 18.0.0 is re-certified as OpenID Connect Provider. We already obtained certification for the OpenID Connect protocol a long time ago with the Keycloak 2.3.0. We now re-certified all the existing configurations (Basic, Implicit, Hybrid, Config, Dynamic) with latest Keycloak 18.0.0 and added certification as a Form Post OP. See the for the details. * Keycloak 18.0.0 is certified as OpenID Connect Logout Provider with all logout profiles (RP-Initiated OP, Session OP, Front-Channel OP, Backchannel OP). See the for the details. * Keycloak 15.0.2 is certified as , which is the extension based on existing FAPI 1 Advanced Final certification, which Keycloak already obtained before. See the for the details. This milestone was achieved due the hard work of the awesome Keycloak community, who contributed lots of features related to OpenID Connect Protocol, OpenID Connect Logout and FAPI. The special Thanks go to the , who helped a lot with the FAPI and OpenID Connect related features and especially to , who is doing an awesome job for the Keycloak project.</content><dc:creator>Marek Posolda</dc:creator></entry><entry><title>What’s new in Ansible Automation Platform 2.2</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/26/whats-new-ansible-automation-platform-22" /><author><name>Don Schenck</name></author><id>b36c1da4-5a9c-4beb-a73b-271f93dd4dcf</id><updated>2022-05-26T15:30:00Z</updated><published>2022-05-26T15:30:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; is a Red Hat offering based on &lt;a href="https://www.ansible.com/overview/it-automation"&gt;Ansible&lt;/a&gt; that allows you to configure your systems via code, and version 2.2 is now generally available. Whether you want to install a framework, deploy an application, or tweak some network settings, Ansible Automation Platform is the easiest way to get the job done.&lt;/p&gt; &lt;p&gt;Ansible Automation Platform works with &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; too, which means you can bring network administration experience from virtual machine (VM) environments to the brave new world of &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here's a list of what's new for Ansible Automation Platform 2.2:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A topology viewer&lt;/li&gt; &lt;li&gt;More powerful developer tooling&lt;/li&gt; &lt;li&gt;Enhancements to network automation&lt;/li&gt; &lt;li&gt;Streamlined integration with &lt;a href="https://www.redhat.com/en/technologies/management/insights"&gt;Red Hat Insights&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evolving support in &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Chain-of-custody tracking via digitally-signed components&lt;/li&gt; &lt;li&gt;An automation services catalog as a self-hosted, on-premises offering&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's dive into the details for each of these.&lt;/p&gt; &lt;h2&gt;Topology viewer&lt;/h2&gt; &lt;p&gt;The Ansible Automation Platform topology viewer (Figure 1) allows you to see your overlay network and the results of Ansible automation. The visual layout makes support and troubleshooting easier while providing you with a big-picture view.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_7.png?itok=EtrLIb_5" width="367" height="388" alt="Screenshot of the topology viewer, which shows the relationships between your resources." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The topology viewer shows the relationships between your resources. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;More powerful developer tooling&lt;/h2&gt; &lt;p&gt;Ansible now has ansible-lint, which works like any good code linter to highlight errors, promote best practices, and reduce mistakes. ansible-lint also acts as an assistant if you're upgrading from an older (1.2) version of Ansible Automation Platform—which has an end-of-life date of September 2023.&lt;/p&gt; &lt;p&gt;There's also ansible-navigator 2.0, which brings other added capabilities to the table as well:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pass-through control of ansible-builder, along with experimental pass-through control of ansible-lint&lt;/li&gt; &lt;li&gt;Simple settings management of modified and active ansible-navigator configurations within Visual Studio Code&lt;/li&gt; &lt;li&gt;Native execution of ad hoc Ansible commands in an execution environment&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To make playbook development easier, a Visual Studio Code extension is &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.ansible"&gt;available for download&lt;/a&gt; (Figure 2). As you would expect, this extension does syntax highlighting and real-time code validation as you type. Autocompletion is built in, and the extension has ansible-lint integrated as well.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/vx.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/vx.png?itok=g8IHqw-M" width="645" height="298" alt="The Visual Studio Marketplace offers a VS Code extension for Ansible." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The Visual Studio Marketplace offers a VS Code extension for Ansible. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Visual Studio Marketplace offers a Visual Studio Code extension for Ansible.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Enhancements to network automation&lt;/h2&gt; &lt;p&gt;Ansible Automation Platform 2.2 brings better performance and resilience to your automation tasks. &lt;a href="https://www.libssh.org"&gt;libssh,&lt;/a&gt; which uses the pylibSSH library, is now used by default for SSH connections.&lt;/p&gt; &lt;p&gt;This new release also uses direct execution by default to improve performance. This means that commands are carried out by the Ansible control node instead of being packaged and executed by the shell.&lt;/p&gt; &lt;p&gt;New resource modules have been released, including &lt;code&gt;snmp_server&lt;/code&gt; and &lt;code&gt;hostname&lt;/code&gt; modules for supported network operating systems: Arista, Cisco, Juniper, and VyOS.&lt;/p&gt; &lt;h2&gt;Streamlined integration with Red Hat Insights&lt;/h2&gt; &lt;p&gt;Ansible Automation Platform 2.2 includes a simpler, more intuitive way to connect your automation data with Red Hat Insights. The insights-client package is responsible for ensuring that connected data for your Ansible Automation Platform infrastructure has also been added to the bundled installer on the Red Hat Customer Portal.&lt;/p&gt; &lt;p&gt;Once you are connected with Red Hat Insights, you can get actionable metrics and dashboards to help identify, troubleshoot, and resolve operational, business, and security issues across your entire ecosystem.&lt;/p&gt; &lt;p&gt;You also gain full visibility into the performance and return on investment (ROI) of your efforts, helping you make more informed decisions to optimize and expand your automation.&lt;/p&gt; &lt;h2&gt;Evolving support in Red Hat Enterprise Linux&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux is keeping up with Ansible Automation Platform in a number of ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ansible Automation Platform 2.2 components are now available in the Red Hat Customer Portal as RPM packages, in both versions 8 and 9 of Red Hat Enterprise Linux.&lt;/li&gt; &lt;li&gt;Ansible Automation Platform 2.2 brings support for version 13 of the PostgreSQL database, which is in Red Hat Enterprise Linux 9. PostgreSQL 13 can be used by multiple Ansible Automation Platform components for improved compatibility and performance.&lt;/li&gt; &lt;li&gt;Ansible Automation Platform 2.2 includes an updated certified Red Hat Enterprise Linux System Roles Collection to automate Red Hat Enterprise Linux 9 instances.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;An eye toward the future…&lt;/h2&gt; &lt;p&gt;The following two features have been introduced in Ansible Automation Platform 2.2 as technology previews. These features provide early access to upcoming product innovations.&lt;/p&gt; &lt;h3&gt;Chain-of-custody tracking via digitally-signed components&lt;/h3&gt; &lt;p&gt;Digital signing of content is an important security feature that allows you to assure that the chain of custody for your assets stays within trusted providers.&lt;/p&gt; &lt;p&gt;Red Hat is also announcing Red Hat Ansible Certified Content: Digitally-signed content from Red Hat and partners to provide end-to-end security from download to deployment.&lt;/p&gt; &lt;p&gt;You can also sign your own content when publishing it to your private automation hub.&lt;/p&gt; &lt;h3&gt;Automation services catalog as a self-hosted, on-premises offering&lt;/h3&gt; &lt;p&gt;The automation services catalog is now a self-hosted, on-premises (private) offering. This gives automation creators and business users self-service access across physical, virtual, cloud, container, and edge environments.&lt;/p&gt; &lt;p&gt;This new iteration of the automation services catalog helps organizations extend the value of their automation to the business user by presenting access to Ansible Automation Platform in a catalog-style format.&lt;/p&gt; &lt;p&gt;With multilevel approval and role-based access control (RBAC), administrators can deploy projects more quickly, with the governance they need to meet compliance and procurement requirements.&lt;/p&gt; &lt;h2&gt;Make the move to 2.2&lt;/h2&gt; &lt;p&gt;You can download the new version of Ansible Automation Platform from &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;our Ansible website&lt;/a&gt;. Try out the newest version and automate everything you do.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/26/whats-new-ansible-automation-platform-22" title="What’s new in Ansible Automation Platform 2.2"&gt;What’s new in Ansible Automation Platform 2.2&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2022-05-26T15:30:00Z</dc:date></entry><entry><title>Filter unwanted notifications in Cryostat 2.1</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/26/filter-unwanted-notifications-cryostat-21" /><author><name>Andrew Azores</name></author><id>16954849-a569-4372-b383-dda3dcee0daa</id><updated>2022-05-26T07:00:00Z</updated><published>2022-05-26T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://cryostat.io"&gt;Cryostat&lt;/a&gt; has always issued notifications when monitoring &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; applications with &lt;a href="https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170"&gt;Java Flight Recorder&lt;/a&gt; (JFK). Version 2.1 of Cryostat has a new implementation and interface for notifications that increases the amount of information offered, enhances the user's control over what is displayed, and improves Cryostat performance.&lt;/p&gt; &lt;h2&gt;Cryostat interface&lt;/h2&gt; &lt;p&gt;Thanks to a new WebSocket interface, Cryostat issues notifications for events that were not reported before. Thus, by default, many more notifications are reported to the user. For instance, the new &lt;a href="https://developers.redhat.com/articles/2022/05/11/how-build-automated-jfr-rules-cryostat-21s-new-ui"&gt;automated rules&lt;/a&gt; feature adds rules to lots of instances at once and emits notifications when each rule is created or deleted, as well as when a rule starts a recording, copies a recording to archive, or prunes an old recording from the archive. The volume of notifications could become especially daunting, as Figure 1 illustrates.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image1_5.png?itok=0-zCTz84" width="666" height="1014" alt="Cryostat can deliver an enormous number of notifications." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Cryostat can deliver an enormous number of notifications. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;To deal with this problem, Cryostat 2.1 gives users the ability to enable and disable the notifications they receive through a new Settings page (Figure 2). The user can select notifications by category or, with a single click, enable or disable all notifications.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image2_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image2_0.png?itok=t4XT14Q8" width="1440" height="594" alt="The Settings view in Cryostat 2.1 includes a Notifications widget with toggles for each notification category." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The Settings view in Cryostat 2.1 includes a Notifications widget with toggles for each notification category. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;For example, to disable all notifications that might be emitted by the background activities of the automated rules, toggle off the following categories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Recording Created&lt;/strong&gt; (emitted when an automated rule creates a new recording on a target)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recording Saved&lt;/strong&gt; (emitted when an automated rule copies an active recording to the archives)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Archived Recording Deleted&lt;/strong&gt; (emitted when an automated rule trims old copies from the archives)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Implementation and performance&lt;/h2&gt; &lt;p&gt;Cryostat uses a WebSocket connection between the Cryostat backend and the web client running in your browser to display notifications when various actions or state changes occur. In Cryostat 2.1, a WebSocket notification is emitted for all conceptual actions and state changes that can occur, whereas previous versions of Cryostat did not include notifications for some types of action or state change. Formatting is also more consistent now, with a detailed title and description for each notification type.&lt;/p&gt; &lt;p&gt;When a notification category is disabled, the messages are still sent over the WebSocket connection, but the toast notification in the web client simply isn't displayed. The user's chosen settings are entirely on the web client side and are persisted in the browser's local storage. The web client still receives and holds the notification information in memory. Therefore, if you re-enable a notification category, any previously emitted notifications in that category will still be available for you to read. This way, Cryostat avoids cluttering the user interface with notifications while still preserving a log of actions and state changes for inspection if needed.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The notification categories do not distinguish between actions performed by an automated rule and those performed by an interactive user by another client via API requests. If you disable these notifications, they will be disabled for all cases, not only when an automated rule performs them.&lt;/p&gt; &lt;p&gt;Web client performance has also been improved by using notifications to update the state of resources such as active recordings, archived recordings, and templates. For example, when an active recording is deleted, the web client now uses the corresponding notification from the backend to update the active recordings list by deleting the recording in question. Other state updates work in a similar manner. Previously, this workflow would have required sending an HTTP GET request to the backend to get the entire, updated list of active recordings and using this list to fully replace the outdated list displayed to the user. Now the web client simply updates a single deleted recording entry.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Cryostat's notification process is much better structured and richer in version 2.1. It requires a bit more preparation in your browser to get the notifications you want and exclude the many others you might find distracting. But consistency and performance improvements create a better user interface.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/26/filter-unwanted-notifications-cryostat-21" title="Filter unwanted notifications in Cryostat 2.1"&gt;Filter unwanted notifications in Cryostat 2.1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andrew Azores</dc:creator><dc:date>2022-05-26T07:00:00Z</dc:date></entry><entry><title type="html">Upgrade Drools version</title><link rel="alternate" href="https://blog.kie.org/2022/05/upgrade-drools-version.html" /><author><name>Toshiya Kobayashi</name></author><id>https://blog.kie.org/2022/05/upgrade-drools-version.html</id><updated>2022-05-26T00:00:00Z</updated><content type="html">We sometimes see questions regarding Drools version upgrade. Most of those questions are about how to change the old API usage in Drools 5 or 6. In this article, I’m going to guide how to change your code to work with the latest Drools version (7.70.0.Final as of now). API If you are using old style APIs, it would be like this. Drools 5 Drools 6 Since Drools 6, we have introduced new KIE APIs. So the old APIs are considered as internal APIs that are not recommended to use in user applications. In Drools 7, technically, we can write in a similar style. However, I write this example not to recommend you write in this style. Just to show a kind of bridge between old APIs and new KIE APIs. In all the above examples, you build a package and give the package to a kbase. In the new KIE APIs, the procedure is encapsulated in KieContainer. Please have a look at the new KIE APIs style. "Basic DRL" "Kjar and Runner" "Programmatically create a Kjar" For details, please refer to the blog post. &gt; If you have all rules in classpath (e.g. inside an application), "Basic DRL" would be the easiest example. If you had rules in serialized PKGs, you would choose "Kjar and Runner" approach. If you want to programmatically manage resource files, "Programmatically create a Kjar" would fit. Some configuration classes might be moved to different packages or even removed. If you have any doubt, feel free to ask in community channels ( , , ). RULES DRL and XLS spreadsheets are basically backward compatible. So you can use rule assets in Drools 7. But there is one caveat, "Property reactivity". Property reactivity was introduced since Drools 5.4 but it was not enabled by default. However, it’s enabled by default since Drools 7. It means you may see that rules react differently when you upgrade the version. In that case, try to disable the feature and check if the different behaviour was caused by property reactivity. If that is the case, you have 2 options. A. Keep property reactivity disabled B. Enable property reactivity and review your rules to meet property reactivity behaviour Property reactivity a powerful feature which allows you to write rules naturally and also provide good performance. Please refer to this section for details. CONCLUSION Upgrading versions is a key maintenance action of open source software use. Please don’t hesitate! The post appeared first on .</content><dc:creator>Toshiya Kobayashi</dc:creator></entry></feed>
